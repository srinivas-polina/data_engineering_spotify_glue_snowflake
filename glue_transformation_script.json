{
	"jobConfig": {
		"name": "spotify_transformation_job1",
		"description": "spotify_transformation_job",
		"role": "arn:aws:iam::011528262092:role/spotify_glue_IAM_role",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 5,
		"maxCapacity": 5,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "spotify_transformation_job.py",
		"scriptLocation": "s3://aws-glue-assets-011528262092-us-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-08-28T21:51:58.147Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-011528262092-us-east-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-011528262092-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nimport boto3\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n\n\n\n\nfrom pyspark.sql.functions import *\nfrom datetime import datetime\n\nfrom awsglue.dynamicframe import DynamicFrame\ns3_path = \"s3://spotify-etl-project-srinivas/raw_data/toprocessed/\"\nsource_dyf = glueContext.create_dynamic_frame_from_options(\n    connection_type=\"s3\",\n    connection_options={\"paths\":[s3_path]},\n    format=\"json\"\n)\n\n\n\n\nspotify_df = source_dyf.toDF()\n\n\n\ndef process_albums(df):\n    df = df.withColumn(\"items\",explode(\"items\")).select(\n         col(\"items.track.album.id\").alias(\"album_id\"),\n         col(\"items.track.album.name\").alias(\"album_name\"),\n         col(\"items.track.album.release_date\").alias(\"release_date\"),\n         col(\"items.track.album.total_tracks\").alias(\"total_tracks\"),\n         col(\"items.track.external_urls.spotify\").alias(\"url\")\n    )\n    return df\n\ndef process_artists(df):\n    #frist explode the items to get individual tracks\n    df_items_exploded = df.select(explode(col(\"items\")).alias(\"items\"))\n    \n    #then explode the artists array within each item to create a row for each artist\n    df_artist_exploded = df_items_exploded.select(explode(col(\"items.track.artists\")).alias(\"artist\"))\n    \n    #now select the artist attributes,ensuring each artist is in its own row\n    df_artists = df_artist_exploded.select(\n      col(\"artist.id\").alias(\"artist_id\"),\n      col(\"artist.name\").alias(\"artist_name\"),\n      col(\"artist.external_urls.spotify\").alias(\"url\")\n    ).drop_duplicates([\"artist_id\"])\n\n    return df_artists\n\ndef process_songs(df):\n    #explode the items array to create a row for each song\n    df_explode = df.select(explode(col(\"items\")).alias(\"items\"))\n    #extract song information from the exploded dataframe\n    df_tracks= df_explode.select(\n        col(\"items.track.id\").alias(\"song_id\"),\n        col(\"items.track.name\").alias(\"song_name\"),\n        col(\"items.track.duration_ms\").alias(\"duration_ms\"),\n        col(\"items.track.popularity\").alias(\"popularity\"),\n        col(\"items.track.external_urls.spotify\").alias(\"url\"),\n        col(\"items.added_at\").alias(\"added_at\"),\n        col(\"items.track.album.id\").alias(\"album_id\"),\n        col(\"items.track.artists\")[0][\"id\"].alias(\"artist_id\")\n    )\n    #convert string dates in song_added to actual date type format\n    df_tracks = df.tracks.withColumn(\"added_at\", to_date(col(\"added_at\")))\n    \n    return df_tracks\ndef process_albums(df):\n    df = df.withColumn(\"items\",explode(\"items\")).select(\n         col(\"items.track.album.id\").alias(\"album_id\"),\n         col(\"items.track.album.name\").alias(\"album_name\"),\n         col(\"items.track.album.release_date\").alias(\"release_date\"),\n         col(\"items.track.album.total_tracks\").alias(\"total_tracks\"),\n         col(\"items.track.external_urls.spotify\").alias(\"url\")\n    )\n    return df\n\ndef process_artists(df):\n    #frist explode the items to get individual tracks\n    df_items_exploded = df.select(explode(col(\"items\")).alias(\"items\"))\n    \n    #then explode the artists array within each item to create a row for each artist\n    df_artist_exploded = df_items_exploded.select(explode(col(\"items.track.artists\")).alias(\"artist\"))\n    \n    #now select the artist attributes,ensuring each artist is in its own row\n    df_artists = df_artist_exploded.select(\n      col(\"artist.id\").alias(\"artist_id\"),\n      col(\"artist.name\").alias(\"artist_name\"),\n      col(\"artist.external_urls.spotify\").alias(\"url\")\n    ).drop_duplicates([\"artist_id\"])\n\n    return df_artists\n\ndef process_songs(df):\n    #explode the items array to create a row for each song\n    df_explode = df.select(explode(col(\"items\")).alias(\"items\"))\n    #extract song information from the exploded dataframe\n    df_tracks= df_explode.select(\n        col(\"items.track.id\").alias(\"song_id\"),\n        col(\"items.track.name\").alias(\"song_name\"),\n        col(\"items.track.duration_ms\").alias(\"duration_ms\"),\n        col(\"items.track.popularity\").alias(\"popularity\"),\n        col(\"items.track.external_urls.spotify\").alias(\"url\"),\n        col(\"items.added_at\").alias(\"added_at\"),\n        col(\"items.track.album.id\").alias(\"album_id\"),\n        col(\"items.track.artists\")[0][\"id\"].alias(\"artist_id\")\n    )\n    #convert string dates in song_added to actual date type format\n    df_tracks = df_tracks.withColumn(\"added_at\", to_date(col(\"added_at\")))\n    \n    return df_tracks\nalbum_df = process_albums(spotify_df)\nalbum_df.show(50)\nartist_df = process_artists(spotify_df)\nartist_df.show(50)\nsongs_df = process_songs(spotify_df)\nsongs_df.show(50)\n#writing this files back to s3 \ndef write_to_s3(df, path_suffix, format_type=\"csv\"):\n    # Convert Spark DataFrame to AWS Glue DynamicFrame\n    dynamic_frame = DynamicFrame.fromDF(df, glueContext, \"dynamic_frame\")\n    \n    # Write DynamicFrame to S3\n    glueContext.write_dynamic_frame.from_options(\n        frame=dynamic_frame,\n        connection_type=\"s3\",\n        connection_options={\"path\": f\"s3://spotify-etl-project-srinivas/transformed_data/{path_suffix}/\"},\n        format=format_type\n    )\n\n\nwrite_to_s3(album_df, \"album_data/album_transformed_{}\".format(datetime.now().strftime(\"%Y-%m-%d\")),\"csv\")\nwrite_to_s3(artist_df, \"artist_data/artist_transformed_{}\".format(datetime.now().strftime(\"%Y-%m-%d\")),\"csv\")\nwrite_to_s3(songs_df, \"song_data/song_transformed_{}\".format(datetime.now().strftime(\"%Y-%m-%d\")),\"csv\")\n\ndef list_s3_objects(bucket, prefix):\n    s3_client = boto3.client('s3')\n    print(f\"Listing objects in bucket: {bucket} with prefix: {prefix}\")\n    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)  # Note the parameter 'Prefix'\n    keys = [content['Key'] for content in response.get('Contents', []) if content['Key'].endswith('.json')]\n    return keys\n    \nbucket_name = \"spotify-etl-project-srinivas\"\nprefix = \"raw_data/toprocessed/\"\nspotify_keys = list_s3_objects(bucket_name,prefix)\n\ndef move_and_delete_files(spotify_keys, Bucket):\n    s3_resource = boto3.resource('s3')\n    for key in spotify_keys:\n        copy_source = {\n            'Bucket': Bucket,\n            'Key': key\n        }\n        \n        #defining the destination key\n        destination_key = 'raw_data/processed/' + key.split(\"/\")[-1]\n        \n        #copy the file to the new location\n        s3_resource.meta.client.copy(copy_source, Bucket, destination_key)\n        \n        #delete the original file\n        s3_resource.Object(Bucket, key).delete()\n        \nmove_and_delete_files(spotify_keys, bucket_name)\n\njob.commit()"
}